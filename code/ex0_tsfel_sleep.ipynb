{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322a53b-c4ab-41a7-90cb-ce4f4183af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tsfel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178b362-2b80-4209-bad8-5f10166aaea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext memory_profiler\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b8419e-c8ee-4b8d-961c-91b53b1b61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d70da5-a602-4e1a-b166-4b2e53f886df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddbb2a4d-bdaf-40fe-becb-61ecb84c8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from functional import seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a568937b-8c1e-49a3-9503-8e256cbdc654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing all existing csv files\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/6f0d59c1-aa9f-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/6fcbea81-95dc-11ea-83ea-832802f41e4e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/3bfa3a21-ab9a-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/41963a21-4c49-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/46aeead1-8c21-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/4c4e3ab1-e9d2-11e9-bee9-d1d3ec387d54/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/50ecc6d1-9d19-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/5644a5d1-86ee-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/5b570da1-ab2a-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/615e31c1-8590-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/6fffcbc1-95dc-11ea-83ea-832802f41e4e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/63e0c771-558c-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/64b238f1-8ad9-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/67887001-95dc-11ea-83ea-832802f41e4e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/697a2de1-95dc-11ea-83ea-832802f41e4e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/6afa1721-95dc-11ea-83ea-832802f41e4e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/6ca4be71-45e7-11eb-9dd4-cb04303e036e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/6e9e6701-95dc-11ea-83ea-832802f41e4e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/70b909a1-95dc-11ea-83ea-832802f41e4e/csv\n",
      "/users/jonvdrdo/jonas/data/Stressy/interim/wave_3_2/daily_data/79fdb011-8695-11eb-9dd4-cb04303e036e/csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/users/jonvdrdo/jonas\")\n",
    "save_dir = data_dir.joinpath(\"data/Stressy/interim/wave_3_2/daily_data/\")\n",
    "raw_data_dir = data_dir.joinpath(\"data/Stressy/raw/wave_3_2/rawdata/\")\n",
    "\n",
    "print(\"removing all existing csv files\")\n",
    "for idx, raw_user_dir in enumerate(\n",
    "    seq(raw_data_dir.iterdir()).filter(lambda x: x.is_dir()), 1\n",
    "):\n",
    "    user_save_dir = save_dir.joinpath(raw_user_dir.name)\n",
    "    user_csv_dir = user_save_dir.joinpath(\"csv\")\n",
    "    if user_csv_dir.is_dir():\n",
    "        print(user_csv_dir)\n",
    "        shutil.rmtree(user_csv_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2cf6a-da22-4110-8ab7-919e274b1586",
   "metadata": {
    "tags": []
   },
   "source": [
    "## tsfel tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e433e5-27c3-4a2a-85f6-64e9f13e67bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage:  96.13 MB\n"
     ]
    }
   ],
   "source": [
    "import tsfel\n",
    "from utils import get_data\n",
    "df_emg = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f464c7-dce9-422d-8c89-705b220295b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "fs = 1000\n",
    "feat_dict = (\n",
    "    {\n",
    "        \"my_custom_set_of_features\": {\n",
    "            \"Min\": {\n",
    "                \"function\": \"tsfel.calc_min\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Max\": {\n",
    "                \"function\": \"tsfel.calc_max\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Mean\": {\n",
    "                \"function\": \"tsfel.calc_mean\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "                \"tag\": \"inertial\",\n",
    "            },\n",
    "            \"Standard deviation\": {\n",
    "                \"function\": \"tsfel.calc_std\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Variance\": {\n",
    "                \"function\": \"tsfel.calc_var\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Skewness\": {\n",
    "                \"function\": \"tsfel.skewness\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Kurtosis\": {\n",
    "                \"function\": \"tsfel.kurtosis\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Slope\": {\n",
    "                \"function\": \"tsfel.slope\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Root mean square\": {\n",
    "                \"function\": \"tsfel.rms\",\n",
    "                \"parameters\": \"\",\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "            \"Total energy\": {\n",
    "                \"function\": \"tsfel.total_energy\",\n",
    "                \"parameters\": {\"fs\": fs},\n",
    "                \"n_features\": 1,\n",
    "                \"use\": \"yes\",\n",
    "            },\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cff25d3-ce44-4f7e-9bf0-7790c0b5ff10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>            \n",
       "              <progress\n",
       "                  value='358'\n",
       "                  max='358',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  358\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n"
     ]
    }
   ],
   "source": [
    "fs = 1000\n",
    "out = tsfel.time_series_features_extractor(\n",
    "    dict_features=feat_dict,\n",
    "    signal_windows=df_emg.values,  # [\"emg\"],#.values,\n",
    "    fs=fs,\n",
    "    window_size=30 * fs,\n",
    "    overlap=2.0 / 3,\n",
    "    header_names=df_emg.columns,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a869633-991d-44d4-bc46-2d4747e7107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emg_Kurtosis</th>\n",
       "      <th>emg_Max</th>\n",
       "      <th>emg_Mean</th>\n",
       "      <th>emg_Min</th>\n",
       "      <th>emg_Root mean square</th>\n",
       "      <th>emg_Skewness</th>\n",
       "      <th>emg_Slope</th>\n",
       "      <th>emg_Standard deviation</th>\n",
       "      <th>emg_Total energy</th>\n",
       "      <th>emg_Variance</th>\n",
       "      <th>...</th>\n",
       "      <th>rio_Kurtosis</th>\n",
       "      <th>rio_Max</th>\n",
       "      <th>rio_Mean</th>\n",
       "      <th>rio_Min</th>\n",
       "      <th>rio_Root mean square</th>\n",
       "      <th>rio_Skewness</th>\n",
       "      <th>rio_Slope</th>\n",
       "      <th>rio_Standard deviation</th>\n",
       "      <th>rio_Total energy</th>\n",
       "      <th>rio_Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.178887</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>0.076752</td>\n",
       "      <td>1.453172e-07</td>\n",
       "      <td>0.011084</td>\n",
       "      <td>0.127446</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>...</td>\n",
       "      <td>4.178887</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>0.076752</td>\n",
       "      <td>1.453172e-07</td>\n",
       "      <td>0.011084</td>\n",
       "      <td>0.127446</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.247341</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>0.010540</td>\n",
       "      <td>0.052679</td>\n",
       "      <td>-7.435104e-08</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.111088</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>...</td>\n",
       "      <td>3.247341</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>0.010540</td>\n",
       "      <td>0.052679</td>\n",
       "      <td>-7.435104e-08</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.111088</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.068759</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.035995</td>\n",
       "      <td>-1.078125</td>\n",
       "      <td>0.153485</td>\n",
       "      <td>4.218277</td>\n",
       "      <td>6.763373e-06</td>\n",
       "      <td>0.149204</td>\n",
       "      <td>23.558321</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>...</td>\n",
       "      <td>34.068759</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.035995</td>\n",
       "      <td>-1.078125</td>\n",
       "      <td>0.153485</td>\n",
       "      <td>4.218277</td>\n",
       "      <td>6.763373e-06</td>\n",
       "      <td>0.149204</td>\n",
       "      <td>23.558321</td>\n",
       "      <td>0.022262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.566366</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>0.158398</td>\n",
       "      <td>-1.312500</td>\n",
       "      <td>0.313075</td>\n",
       "      <td>1.053389</td>\n",
       "      <td>1.850948e-05</td>\n",
       "      <td>0.270048</td>\n",
       "      <td>98.019014</td>\n",
       "      <td>0.072926</td>\n",
       "      <td>...</td>\n",
       "      <td>4.566366</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>0.158398</td>\n",
       "      <td>-1.312500</td>\n",
       "      <td>0.313075</td>\n",
       "      <td>1.053389</td>\n",
       "      <td>1.850948e-05</td>\n",
       "      <td>0.270048</td>\n",
       "      <td>98.019014</td>\n",
       "      <td>0.072926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.114830</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>0.282729</td>\n",
       "      <td>-1.312500</td>\n",
       "      <td>0.381529</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>1.362858e-05</td>\n",
       "      <td>0.256180</td>\n",
       "      <td>145.568899</td>\n",
       "      <td>0.065628</td>\n",
       "      <td>...</td>\n",
       "      <td>5.114830</td>\n",
       "      <td>1.812500</td>\n",
       "      <td>0.282729</td>\n",
       "      <td>-1.312500</td>\n",
       "      <td>0.381529</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>1.362858e-05</td>\n",
       "      <td>0.256180</td>\n",
       "      <td>145.568899</td>\n",
       "      <td>0.065628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.398492</td>\n",
       "      <td>1.484375</td>\n",
       "      <td>0.506242</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>0.549090</td>\n",
       "      <td>-0.133734</td>\n",
       "      <td>1.012667e-05</td>\n",
       "      <td>0.212646</td>\n",
       "      <td>301.509440</td>\n",
       "      <td>0.045218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398492</td>\n",
       "      <td>1.484375</td>\n",
       "      <td>0.506242</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>0.549090</td>\n",
       "      <td>-0.133734</td>\n",
       "      <td>1.012667e-05</td>\n",
       "      <td>0.212646</td>\n",
       "      <td>301.509440</td>\n",
       "      <td>0.045218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>15.259459</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.632273</td>\n",
       "      <td>-1.062500</td>\n",
       "      <td>0.662533</td>\n",
       "      <td>0.342239</td>\n",
       "      <td>3.009870e-06</td>\n",
       "      <td>0.197939</td>\n",
       "      <td>438.964217</td>\n",
       "      <td>0.039180</td>\n",
       "      <td>...</td>\n",
       "      <td>15.259459</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.632273</td>\n",
       "      <td>-1.062500</td>\n",
       "      <td>0.662533</td>\n",
       "      <td>0.342239</td>\n",
       "      <td>3.009870e-06</td>\n",
       "      <td>0.197939</td>\n",
       "      <td>438.964217</td>\n",
       "      <td>0.039180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>5.770678</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.546607</td>\n",
       "      <td>-1.062500</td>\n",
       "      <td>0.598843</td>\n",
       "      <td>0.704499</td>\n",
       "      <td>-1.033203e-05</td>\n",
       "      <td>0.244609</td>\n",
       "      <td>358.624381</td>\n",
       "      <td>0.059833</td>\n",
       "      <td>...</td>\n",
       "      <td>5.770678</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.546607</td>\n",
       "      <td>-1.062500</td>\n",
       "      <td>0.598843</td>\n",
       "      <td>0.704499</td>\n",
       "      <td>-1.033203e-05</td>\n",
       "      <td>0.244609</td>\n",
       "      <td>358.624381</td>\n",
       "      <td>0.059833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>6.027836</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.610461</td>\n",
       "      <td>-1.890625</td>\n",
       "      <td>0.675592</td>\n",
       "      <td>0.310122</td>\n",
       "      <td>2.920637e-06</td>\n",
       "      <td>0.289416</td>\n",
       "      <td>456.439653</td>\n",
       "      <td>0.083762</td>\n",
       "      <td>...</td>\n",
       "      <td>6.027836</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.610461</td>\n",
       "      <td>-1.890625</td>\n",
       "      <td>0.675592</td>\n",
       "      <td>0.310122</td>\n",
       "      <td>2.920637e-06</td>\n",
       "      <td>0.289416</td>\n",
       "      <td>456.439653</td>\n",
       "      <td>0.083762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>6.060607</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.495109</td>\n",
       "      <td>-1.890625</td>\n",
       "      <td>0.560601</td>\n",
       "      <td>1.040788</td>\n",
       "      <td>1.756395e-06</td>\n",
       "      <td>0.262946</td>\n",
       "      <td>314.284402</td>\n",
       "      <td>0.069141</td>\n",
       "      <td>...</td>\n",
       "      <td>6.060607</td>\n",
       "      <td>1.984375</td>\n",
       "      <td>0.495109</td>\n",
       "      <td>-1.890625</td>\n",
       "      <td>0.560601</td>\n",
       "      <td>1.040788</td>\n",
       "      <td>1.756395e-06</td>\n",
       "      <td>0.262946</td>\n",
       "      <td>314.284402</td>\n",
       "      <td>0.069141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>358 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emg_Kurtosis   emg_Max  emg_Mean   emg_Min  emg_Root mean square  \\\n",
       "0        4.178887  0.062500 -0.002141 -0.062500              0.011289   \n",
       "1        3.247341  0.062500 -0.001375 -0.046875              0.010540   \n",
       "2       34.068759  1.531250  0.035995 -1.078125              0.153485   \n",
       "3        4.566366  1.812500  0.158398 -1.312500              0.313075   \n",
       "4        5.114830  1.812500  0.282729 -1.312500              0.381529   \n",
       "..            ...       ...       ...       ...                   ...   \n",
       "353      0.398492  1.484375  0.506242 -0.328125              0.549090   \n",
       "354     15.259459  1.984375  0.632273 -1.062500              0.662533   \n",
       "355      5.770678  1.984375  0.546607 -1.062500              0.598843   \n",
       "356      6.027836  1.984375  0.610461 -1.890625              0.675592   \n",
       "357      6.060607  1.984375  0.495109 -1.890625              0.560601   \n",
       "\n",
       "     emg_Skewness     emg_Slope  emg_Standard deviation  emg_Total energy  \\\n",
       "0        0.076752  1.453172e-07                0.011084          0.127446   \n",
       "1        0.052679 -7.435104e-08                0.010450          0.111088   \n",
       "2        4.218277  6.763373e-06                0.149204         23.558321   \n",
       "3        1.053389  1.850948e-05                0.270048         98.019014   \n",
       "4        0.002528  1.362858e-05                0.256180        145.568899   \n",
       "..            ...           ...                     ...               ...   \n",
       "353     -0.133734  1.012667e-05                0.212646        301.509440   \n",
       "354      0.342239  3.009870e-06                0.197939        438.964217   \n",
       "355      0.704499 -1.033203e-05                0.244609        358.624381   \n",
       "356      0.310122  2.920637e-06                0.289416        456.439653   \n",
       "357      1.040788  1.756395e-06                0.262946        314.284402   \n",
       "\n",
       "     emg_Variance  ...  rio_Kurtosis   rio_Max  rio_Mean   rio_Min  \\\n",
       "0        0.000123  ...      4.178887  0.062500 -0.002141 -0.062500   \n",
       "1        0.000109  ...      3.247341  0.062500 -0.001375 -0.046875   \n",
       "2        0.022262  ...     34.068759  1.531250  0.035995 -1.078125   \n",
       "3        0.072926  ...      4.566366  1.812500  0.158398 -1.312500   \n",
       "4        0.065628  ...      5.114830  1.812500  0.282729 -1.312500   \n",
       "..            ...  ...           ...       ...       ...       ...   \n",
       "353      0.045218  ...      0.398492  1.484375  0.506242 -0.328125   \n",
       "354      0.039180  ...     15.259459  1.984375  0.632273 -1.062500   \n",
       "355      0.059833  ...      5.770678  1.984375  0.546607 -1.062500   \n",
       "356      0.083762  ...      6.027836  1.984375  0.610461 -1.890625   \n",
       "357      0.069141  ...      6.060607  1.984375  0.495109 -1.890625   \n",
       "\n",
       "     rio_Root mean square  rio_Skewness     rio_Slope  rio_Standard deviation  \\\n",
       "0                0.011289      0.076752  1.453172e-07                0.011084   \n",
       "1                0.010540      0.052679 -7.435104e-08                0.010450   \n",
       "2                0.153485      4.218277  6.763373e-06                0.149204   \n",
       "3                0.313075      1.053389  1.850948e-05                0.270048   \n",
       "4                0.381529      0.002528  1.362858e-05                0.256180   \n",
       "..                    ...           ...           ...                     ...   \n",
       "353              0.549090     -0.133734  1.012667e-05                0.212646   \n",
       "354              0.662533      0.342239  3.009870e-06                0.197939   \n",
       "355              0.598843      0.704499 -1.033203e-05                0.244609   \n",
       "356              0.675592      0.310122  2.920637e-06                0.289416   \n",
       "357              0.560601      1.040788  1.756395e-06                0.262946   \n",
       "\n",
       "     rio_Total energy  rio_Variance  \n",
       "0            0.127446      0.000123  \n",
       "1            0.111088      0.000109  \n",
       "2           23.558321      0.022262  \n",
       "3           98.019014      0.072926  \n",
       "4          145.568899      0.065628  \n",
       "..                ...           ...  \n",
       "353        301.509440      0.045218  \n",
       "354        438.964217      0.039180  \n",
       "355        358.624381      0.059833  \n",
       "356        456.439653      0.083762  \n",
       "357        314.284402      0.069141  \n",
       "\n",
       "[358 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574b8c2-074c-43ef-9323-359a335e452e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9736a-d8e8-44e1-99ef-219d1ba8c2f4",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## tsflex tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23875fd-90da-4f57-9383-2c94c1f31fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from tsflex.features import FeatureCollection, NumpyFuncWrapper\n",
    "from tsflex.features import FeatureDescriptor, MultipleFeatureDescriptors\n",
    "from tsflex.features.strided_rolling import StridedRolling\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "quantiles = [0.25, 0.5, 0.75]\n",
    "\n",
    "\n",
    "def type_wrapper(x: np.ndarray, type_wrapped_func, **kwargs):\n",
    "    return type_wrapped_func(x, **kwargs).astype(x.dtype)\n",
    "\n",
    "\n",
    "# -- 2. in-line functions\n",
    "#    You can define your functions locally; these will serialize flawleslly\n",
    "def slope(x):\n",
    "    return np.polyfit(np.arange(0, len(x)), x, 1)[0]\n",
    "\n",
    "\n",
    "f_slope = NumpyFuncWrapper(type_wrapper, output_names=\"slope\", type_wrapped_func=slope)\n",
    "\n",
    "# -- 3. Lambda's\n",
    "#    Or even use lambda's and other modules' functions\n",
    "def rms(x): return np.sqrt(np.mean(x**2))\n",
    "f_rms = NumpyFuncWrapper(rms, output_names=\"rms\")\n",
    "# f_rms = NumpyFuncWrapper(lambda x: np.sqrt(np.mean(x ** 2)), output_names=\"rms\")\n",
    "f_area = NumpyFuncWrapper(np.sum, output_names=\"area\")\n",
    "\n",
    "# (For convenience) we store the constructed `NumpyFuncWrappers` in a list\n",
    "segment_funcs = [\n",
    "    np.min,\n",
    "    np.max,\n",
    "    np.mean,\n",
    "    np.std,\n",
    "    np.var,\n",
    "    ss.skew,\n",
    "    ss.kurtosis,\n",
    "    f_slope,\n",
    "    f_rms,\n",
    "    f_area,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd861d2-1dbb-491a-b5fc-2fa18b8f9610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage:  96.13 MB\n"
     ]
    }
   ],
   "source": [
    "from utils import get_data\n",
    "df_emg = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ca99378-4f8e-4370-88de-88257e3e4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "proj_dir = '../..'\n",
    "\n",
    "def dummy_data() -> pd.DataFrame:\n",
    "    df1 = pd.read_parquet(proj_dir + \"/examples/data/empatica/gsr.parquet\", engine='fastparquet')\n",
    "    df2 = pd.read_parquet(proj_dir + \"/examples/data/empatica/tmp.parquet\", engine='fastparquet')\n",
    "    df3 = pd.read_parquet(proj_dir + \"/examples/data/empatica/acc.parquet\", engine='fastparquet')\n",
    "    df = pd.merge(df1, df2, how=\"inner\", on=\"timestamp\")\n",
    "    df = pd.merge(df, df3, how=\"inner\", on=\"timestamp\")\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fdda193-b7b1-475c-9677-2d623db6eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = dummy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02b82527-d9f8-4fdf-b923-ce5a80627445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA</th>\n",
       "      <th>TMP</th>\n",
       "      <th>ACC_x</th>\n",
       "      <th>ACC_y</th>\n",
       "      <th>ACC_z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-06-13 14:22:13+02:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>382.209991</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 14:22:13.250000+02:00</th>\n",
       "      <td>0.400309</td>\n",
       "      <td>382.209991</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 14:22:13.500000+02:00</th>\n",
       "      <td>0.475767</td>\n",
       "      <td>382.209991</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 14:22:13.750000+02:00</th>\n",
       "      <td>0.485999</td>\n",
       "      <td>382.209991</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 14:22:14+02:00</th>\n",
       "      <td>0.488557</td>\n",
       "      <td>27.610001</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 16:28:01.750000+02:00</th>\n",
       "      <td>1.341947</td>\n",
       "      <td>30.990000</td>\n",
       "      <td>9</td>\n",
       "      <td>-4</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 16:28:02+02:00</th>\n",
       "      <td>1.345784</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>11</td>\n",
       "      <td>-7</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 16:28:02.250000+02:00</th>\n",
       "      <td>1.348342</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>9</td>\n",
       "      <td>-12</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 16:28:02.500000+02:00</th>\n",
       "      <td>1.350900</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-13 16:28:02.750000+02:00</th>\n",
       "      <td>1.339390</td>\n",
       "      <td>30.930000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       EDA         TMP  ACC_x  ACC_y  ACC_z\n",
       "timestamp                                                                  \n",
       "2017-06-13 14:22:13+02:00         0.000000  382.209991      0      5     63\n",
       "2017-06-13 14:22:13.250000+02:00  0.400309  382.209991      0      5     63\n",
       "2017-06-13 14:22:13.500000+02:00  0.475767  382.209991     -1      4     63\n",
       "2017-06-13 14:22:13.750000+02:00  0.485999  382.209991     -1      4     62\n",
       "2017-06-13 14:22:14+02:00         0.488557   27.610001     -1      4     63\n",
       "...                                    ...         ...    ...    ...    ...\n",
       "2017-06-13 16:28:01.750000+02:00  1.341947   30.990000      9     -4     53\n",
       "2017-06-13 16:28:02+02:00         1.345784   30.930000     11     -7     63\n",
       "2017-06-13 16:28:02.250000+02:00  1.348342   30.930000      9    -12     60\n",
       "2017-06-13 16:28:02.500000+02:00  1.350900   30.930000      3     -3     61\n",
       "2017-06-13 16:28:02.750000+02:00  1.339390   30.930000      1      1     62\n",
       "\n",
       "[30200 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cdb5d69f-4979-40f3-8cc8-5d5dbebed6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(s1, s2):\n",
    "    min_len = min(len(s1), len(s2))\n",
    "    s1 = s1[:min_len]\n",
    "    s2 = s2[:min_len]\n",
    "    return np.corrcoef(s1, s2)[0][-1].astype(s1.dtype)\n",
    "\n",
    "\n",
    "fc = FeatureCollection(\n",
    "    feature_descriptors=[\n",
    "        FeatureDescriptor(\n",
    "            function=NumpyFuncWrapper(func=corr, output_names=\"corrcoef\"),\n",
    "            series_name=(\"EDA\", \"TMP\"),\n",
    "            window=\"70s\",\n",
    "            stride=\"20s\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2757f222-53cc-4ace-80c8-94e5969fe15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q=0: 280.0, q=0.1: 280.0, q=0.5: 280.0, q=0.9: 280.0, q=1: 280.0\n",
      "q=0: 280.0, q=0.1: 280.0, q=0.5: 280.0, q=0.9: 280.0, q=1: 280.0\n"
     ]
    }
   ],
   "source": [
    "out = fc.calculate(dd, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdefab08-e8f5-42e3-9c1d-5dee1bf9b263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.columns[0] == 'EDA|TMP__corrcoef__w=30s_s=30s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a233789-8b57-4632-b3ab-8639d5a898fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.__repr__() == \"EDA|TMP: (\\n\\twin: 30s   , stride: 10s: [\\n\\t\\tFeatureDescriptor - func: NumpyFuncWrapper(corr, ['corrcoef'], {}),\\n\\t]\\n)\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac5f84-dee9-47f5-944b-895e58bd54e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### stroll tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1c92c6a-bad9-420b-8305-372e53c695a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(10)\n",
    "b = np.random.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d424f2aa-6075-411d-8301-b1a6642072f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11939057098403141"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(a, b).astype(a.dtype)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7793a48f-9deb-43ab-812c-a50c5124ebfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.11939057],\n",
       "       [-0.11939057,  1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5488858-8861-43c9-888d-51bf2e53834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e417783-379a-4032-92b8-309cf4261a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsflex.utils.time import parse_time_arg\n",
    "\n",
    "window= parse_time_arg('30s')\n",
    "stride = parse_time_arg('10s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff39d1-a825-49f2-a439-142b98ef57e3",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "#### __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b644643-d6b5-4576-b279-8372a83ece6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: ../../tsflex/features/strided_rolling.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurences   Line Contents\n",
       "============================================================\n",
       "    76    301.4 MiB    301.4 MiB           1       def __init__(\n",
       "    77                                                     self,\n",
       "    78                                                     data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],\n",
       "    79                                                     window: pd.Timedelta,\n",
       "    80                                                     stride: pd.Timedelta,\n",
       "    81                                                     window_idx: Optional[str] = \"end\",\n",
       "    82                                                     bound_method: Optional[str] = \"inner\",\n",
       "    83                                                     approve_sparsity: Optional[bool] = False,\n",
       "    84                                             ):\n",
       "    85    301.4 MiB      0.0 MiB           1           self.window: pd.Timedelta = window\n",
       "    86    301.4 MiB      0.0 MiB           1           self.stride: pd.Timedelta = stride\n",
       "    87                                         \n",
       "    88                                                 # 0. standardize the input\n",
       "    89    301.4 MiB      0.0 MiB           1           series_list: List[pd.Series] = to_series_list(data)\n",
       "    90    301.4 MiB      0.0 MiB           8           self.series_key: Tuple[str] = tuple([str(s.name) for s in series_list])\n",
       "    91                                         \n",
       "    92                                                 # 1. Determine the bounds\n",
       "    93    301.4 MiB      0.0 MiB           1           t_start, t_end = self._determine_bounds(series_list, bound_method)\n",
       "    94                                         \n",
       "    95                                                 # And slice **all** the series to these tightest bounds\n",
       "    96    301.4 MiB      0.0 MiB           1           assert (t_end - t_start) > window\n",
       "    97    301.4 MiB      0.0 MiB           1           if len(series_list) > 1:\n",
       "    98    301.4 MiB      0.0 MiB           8               series_list = [s[t_start:t_end] for s in series_list]\n",
       "    99                                         \n",
       "   100                                                 # 2. Create the time_index which will be used for DataFrame reconstruction\n",
       "   101                                                 # 2.1 - and adjust the time_index\n",
       "   102                                                 # note: this code can also be placed in the `apply_func` method (if we want to\n",
       "   103                                                 #  make the bound window-idx setting feature specific).\n",
       "   104    301.4 MiB      0.0 MiB           1           if window_idx == \"end\":\n",
       "   105    301.4 MiB      0.0 MiB           1               window_idx_offset = window\n",
       "   106                                                 elif window_idx == \"middle\":\n",
       "   107                                                     window_idx_offset = window / 2\n",
       "   108                                                 elif window_idx == \"begin\":\n",
       "   109                                                     window_idx_offset = 0\n",
       "   110                                                 else:\n",
       "   111                                                     raise ValueError(\n",
       "   112                                                         f\"window index {window_idx} must be either of: \"\n",
       "   113                                                         \"['end', 'middle', 'begin']\"\n",
       "   114                                                     )\n",
       "   115                                         \n",
       "   116                                                 # use closed = left to exclude 'end' if it falls on the boundary\n",
       "   117                                                 # note: the index automatically takes the timezone of `t_start` & `t_end`\n",
       "   118                                                 # note: the index-name of the first passed series will be used\n",
       "   119    301.4 MiB      0.0 MiB           1           self.index = pd.date_range(\n",
       "   120    301.4 MiB      0.0 MiB           1               start=t_start + window_idx_offset,\n",
       "   121    301.4 MiB      0.0 MiB           1               end=t_end - window + window_idx_offset,\n",
       "   122    301.4 MiB      0.0 MiB           1               freq=stride,\n",
       "   123    301.4 MiB      0.0 MiB           1               name=series_list[0].index.name\n",
       "   124                                                 )\n",
       "   125                                         \n",
       "   126                                                 # ---------- Efficient numpy code -------\n",
       "   127                                                 # 1. Convert everything to int64\n",
       "   128    301.4 MiB      0.0 MiB           1           np_start = t_start.to_datetime64()\n",
       "   129    301.4 MiB      0.0 MiB           1           np_window = self.window.to_timedelta64()\n",
       "   130    301.4 MiB      0.0 MiB           1           np_stride = self.stride.to_timedelta64()\n",
       "   131                                         \n",
       "   132                                                 # 2. Precompute the start & end times (these remain the same for each series)\n",
       "   133                                                 # note: this if equivalent to:\n",
       "   134                                                 #   if `window` == 'begin\":\n",
       "   135                                                 #       start_times = self.index.values\n",
       "   136    301.4 MiB      0.0 MiB           1           np_start_times = np.arange(\n",
       "   137    301.4 MiB      0.0 MiB           1               start=np_start, stop=np_start + (len(self.index) * np_stride),\n",
       "   138    301.4 MiB      0.0 MiB           1               step=np_stride,\n",
       "   139    301.4 MiB      0.0 MiB           1               dtype=np.datetime64,\n",
       "   140                                                 )\n",
       "   141    301.4 MiB      0.0 MiB           1           np_end_times = np_start_times + np_window\n",
       "   142                                         \n",
       "   143    301.4 MiB      0.0 MiB           1           self.series_containers: List[StridedRolling._NumpySeriesContainer] = []\n",
       "   144    301.4 MiB      0.0 MiB           6           for series in series_list:\n",
       "   145                                                     # create a non-writeable view of the series\n",
       "   146    301.4 MiB      0.0 MiB           5               np_series = series.values\n",
       "   147    301.4 MiB      0.0 MiB           5               np_series.flags.writeable = False\n",
       "   148                                         \n",
       "   149    301.4 MiB      0.0 MiB           5               np_idx_times = series.index.values\n",
       "   150    301.4 MiB      0.0 MiB           5               self.series_containers.append(\n",
       "   151    301.4 MiB      0.0 MiB           5                   StridedRolling._NumpySeriesContainer(\n",
       "   152                                                             # TODO: maybe save the pd.Series instead of the np.series\n",
       "   153    301.4 MiB      0.0 MiB           5                       values=np_series,\n",
       "   154                                                             # the slicing will be performed on [ t_start, t_end [\n",
       "   155                                                             # TODO: this can maybe be optimized -> further look into this\n",
       "   156                                                             # np_idx_times, np_start_times, & np_end_times are all sorted!\n",
       "   157                                                             # as we assume & check that the time index is monotonically\n",
       "   158                                                             # increasing & the latter 2 are created using `np.arange()`\n",
       "   159    301.4 MiB      0.0 MiB           5                       start_indexes=np.searchsorted(np_idx_times, np_start_times, \"left\"),\n",
       "   160    301.4 MiB      0.0 MiB           5                       end_indexes=np.searchsorted(np_idx_times, np_end_times, \"left\"),\n",
       "   161                                                         )\n",
       "   162                                                     )\n",
       "   163                                         \n",
       "   164    301.4 MiB      0.0 MiB           5               if not approve_sparsity:\n",
       "   165    301.4 MiB      0.0 MiB           5                   last_container = self.series_containers[-1]\n",
       "   166    301.4 MiB      0.0 MiB           5                   qs = [0, 0.1, 0.5, 0.9, 1]\n",
       "   167    301.4 MiB      0.0 MiB           5                   series_idx_stats = np.quantile(\n",
       "   168    301.4 MiB      0.0 MiB           5                       last_container.end_indexes - last_container.start_indexes, q=qs\n",
       "   169                                                         )\n",
       "   170    301.4 MiB      0.0 MiB          40                   q_str = \", \".join([f\"q={q}: {v}\" for q, v in zip(qs, series_idx_stats)])\n",
       "   171    301.4 MiB      0.0 MiB           5                   if series_idx_stats[0] != series_idx_stats[1]:  # min != max\n",
       "   172                                                             warnings.warn(\n",
       "   173                                                                 f\"There are gaps in the time-series {series.name}; \"\n",
       "   174                                                                 + f\"\\n \\t Quantiles of nb values in window: {q_str}\",\n",
       "   175                                                                 RuntimeWarning,\n",
       "   176                                                             )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f StridedRolling.__init__ StridedRolling(data=df_emg, window=window, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799da723-a9db-466f-b782-3ad8603f53c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.008254 s\n",
       "File: ../../tsflex/features/strided_rolling.py\n",
       "Function: __init__ at line 76\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    76                                               def __init__(\n",
       "    77                                                       self,\n",
       "    78                                                       data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],\n",
       "    79                                                       window: pd.Timedelta,\n",
       "    80                                                       stride: pd.Timedelta,\n",
       "    81                                                       window_idx: Optional[str] = \"end\",\n",
       "    82                                                       bound_method: Optional[str] = \"inner\",\n",
       "    83                                                       approve_sparsity: Optional[bool] = False,\n",
       "    84                                               ):\n",
       "    85         1          6.0      6.0      0.1          self.window: pd.Timedelta = window\n",
       "    86         1          2.0      2.0      0.0          self.stride: pd.Timedelta = stride\n",
       "    87                                           \n",
       "    88                                                   # 0. standardize the input\n",
       "    89         1        145.0    145.0      1.8          series_list: List[pd.Series] = to_series_list(data)\n",
       "    90         1         14.0     14.0      0.2          self.series_key: Tuple[str] = tuple([str(s.name) for s in series_list])\n",
       "    91                                           \n",
       "    92                                                   # 1. Determine the bounds\n",
       "    93         1        468.0    468.0      5.7          t_start, t_end = self._determine_bounds(series_list, bound_method)\n",
       "    94                                           \n",
       "    95                                                   # And slice **all** the series to these tightest bounds\n",
       "    96         1        115.0    115.0      1.4          assert (t_end - t_start) > window\n",
       "    97         1          4.0      4.0      0.0          if len(series_list) > 1:\n",
       "    98         1       3912.0   3912.0     47.4              series_list = [s[t_start:t_end] for s in series_list]\n",
       "    99                                           \n",
       "   100                                                   # 2. Create the time_index which will be used for DataFrame reconstruction\n",
       "   101                                                   # 2.1 - and adjust the time_index\n",
       "   102                                                   # note: this code can also be placed in the `apply_func` method (if we want to\n",
       "   103                                                   #  make the bound window-idx setting feature specific).\n",
       "   104         1          3.0      3.0      0.0          if window_idx == \"end\":\n",
       "   105         1          3.0      3.0      0.0              window_idx_offset = window\n",
       "   106                                                   elif window_idx == \"middle\":\n",
       "   107                                                       window_idx_offset = window / 2\n",
       "   108                                                   elif window_idx == \"begin\":\n",
       "   109                                                       window_idx_offset = 0\n",
       "   110                                                   else:\n",
       "   111                                                       raise ValueError(\n",
       "   112                                                           f\"window index {window_idx} must be either of: \"\n",
       "   113                                                           \"['end', 'middle', 'begin']\"\n",
       "   114                                                       )\n",
       "   115                                           \n",
       "   116                                                   # use closed = left to exclude 'end' if it falls on the boundary\n",
       "   117                                                   # note: the index automatically takes the timezone of `t_start` & `t_end`\n",
       "   118                                                   # note: the index-name of the first passed series will be used\n",
       "   119         1          7.0      7.0      0.1          self.index = pd.date_range(\n",
       "   120         1         22.0     22.0      0.3              start=t_start + window_idx_offset,\n",
       "   121         1         56.0     56.0      0.7              end=t_end - window + window_idx_offset,\n",
       "   122         1          3.0      3.0      0.0              freq=stride,\n",
       "   123         1        463.0    463.0      5.6              name=series_list[0].index.name\n",
       "   124                                                   )\n",
       "   125                                           \n",
       "   126                                                   # ---------- Efficient numpy code -------\n",
       "   127                                                   # 1. Convert everything to int64\n",
       "   128         1          9.0      9.0      0.1          np_start = t_start.to_datetime64()\n",
       "   129         1          6.0      6.0      0.1          np_window = self.window.to_timedelta64()\n",
       "   130         1          3.0      3.0      0.0          np_stride = self.stride.to_timedelta64()\n",
       "   131                                           \n",
       "   132                                                   # 2. Precompute the start & end times (these remain the same for each series)\n",
       "   133                                                   # note: this if equivalent to:\n",
       "   134                                                   #   if `window` == 'begin\":\n",
       "   135                                                   #       start_times = self.index.values\n",
       "   136         1          2.0      2.0      0.0          np_start_times = np.arange(\n",
       "   137         1         50.0     50.0      0.6              start=np_start, stop=np_start + (len(self.index) * np_stride),\n",
       "   138         1          3.0      3.0      0.0              step=np_stride,\n",
       "   139         1         14.0     14.0      0.2              dtype=np.datetime64,\n",
       "   140                                                   )\n",
       "   141         1         11.0     11.0      0.1          np_end_times = np_start_times + np_window\n",
       "   142                                           \n",
       "   143         1          3.0      3.0      0.0          self.series_containers: List[StridedRolling._NumpySeriesContainer] = []\n",
       "   144         6         16.0      2.7      0.2          for series in series_list:\n",
       "   145                                                       # create a non-writeable view of the series\n",
       "   146         5         42.0      8.4      0.5              np_series = series.values\n",
       "   147         5         31.0      6.2      0.4              np_series.flags.writeable = False\n",
       "   148                                           \n",
       "   149         5         22.0      4.4      0.3              np_idx_times = series.index.values\n",
       "   150         5         14.0      2.8      0.2              self.series_containers.append(\n",
       "   151         5         11.0      2.2      0.1                  StridedRolling._NumpySeriesContainer(\n",
       "   152                                                               # TODO: maybe save the pd.Series instead of the np.series\n",
       "   153         5         14.0      2.8      0.2                      values=np_series,\n",
       "   154                                                               # the slicing will be performed on [ t_start, t_end [\n",
       "   155                                                               # TODO: this can maybe be optimized -> further look into this\n",
       "   156                                                               # np_idx_times, np_start_times, & np_end_times are all sorted!\n",
       "   157                                                               # as we assume & check that the time index is monotonically\n",
       "   158                                                               # increasing & the latter 2 are created using `np.arange()`\n",
       "   159         5        500.0    100.0      6.1                      start_indexes=np.searchsorted(np_idx_times, np_start_times, \"left\"),\n",
       "   160         5        439.0     87.8      5.3                      end_indexes=np.searchsorted(np_idx_times, np_end_times, \"left\"),\n",
       "   161                                                           )\n",
       "   162                                                       )\n",
       "   163                                           \n",
       "   164         5         14.0      2.8      0.2              if not approve_sparsity:\n",
       "   165         5         15.0      3.0      0.2                  last_container = self.series_containers[-1]\n",
       "   166         5         13.0      2.6      0.2                  qs = [0, 0.1, 0.5, 0.9, 1]\n",
       "   167         5         13.0      2.6      0.2                  series_idx_stats = np.quantile(\n",
       "   168         5       1643.0    328.6     19.9                      last_container.end_indexes - last_container.start_indexes, q=qs\n",
       "   169                                                           )\n",
       "   170         5        121.0     24.2      1.5                  q_str = \", \".join([f\"q={q}: {v}\" for q, v in zip(qs, series_idx_stats)])\n",
       "   171         5         22.0      4.4      0.3                  if series_idx_stats[0] != series_idx_stats[1]:  # min != max\n",
       "   172                                                               warnings.warn(\n",
       "   173                                                                   f\"There are gaps in the time-series {series.name}; \"\n",
       "   174                                                                   + f\"\\n \\t Quantiles of nb values in window: {q_str}\",\n",
       "   175                                                                   RuntimeWarning,\n",
       "   176                                                               )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f StridedRolling.__init__ StridedRolling(data=df_emg, window=window, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32763da-3c68-43c9-b84d-b0d0c94e6b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.001775 s\n",
       "File: ../../tsflex/features/strided_rolling.py\n",
       "Function: __init__ at line 76\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    76                                               def __init__(\n",
       "    77                                                       self,\n",
       "    78                                                       data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],\n",
       "    79                                                       window: pd.Timedelta,\n",
       "    80                                                       stride: pd.Timedelta,\n",
       "    81                                                       window_idx: Optional[str] = \"end\",\n",
       "    82                                                       bound_method: Optional[str] = \"inner\",\n",
       "    83                                                       approve_sparsity: Optional[bool] = False,\n",
       "    84                                               ):\n",
       "    85         1          6.0      6.0      0.3          self.window: pd.Timedelta = window\n",
       "    86         1          3.0      3.0      0.2          self.stride: pd.Timedelta = stride\n",
       "    87                                           \n",
       "    88                                                   # 0. standardize the input\n",
       "    89         1         14.0     14.0      0.8          series_list: List[pd.Series] = to_series_list(data)\n",
       "    90         1          9.0      9.0      0.5          self.series_key: Tuple[str] = tuple([str(s.name) for s in series_list])\n",
       "    91                                           \n",
       "    92                                                   # 1. Determine the bounds\n",
       "    93         1        149.0    149.0      8.4          t_start, t_end = self._determine_bounds(series_list, bound_method)\n",
       "    94                                           \n",
       "    95                                                   # And slice **all** the series to these tightest bounds\n",
       "    96         1         99.0     99.0      5.6          assert (t_end - t_start) > window\n",
       "    97         1          3.0      3.0      0.2          if len(series_list) > 1:\n",
       "    98                                                       series_list = [s[t_start:t_end] for s in series_list]\n",
       "    99                                           \n",
       "   100                                                   # 2. Create the time_index which will be used for DataFrame reconstruction\n",
       "   101                                                   # 2.1 - and adjust the time_index\n",
       "   102                                                   # note: this code can also be placed in the `apply_func` method (if we want to\n",
       "   103                                                   #  make the bound window-idx setting feature specific).\n",
       "   104         1          3.0      3.0      0.2          if window_idx == \"end\":\n",
       "   105         1          2.0      2.0      0.1              window_idx_offset = window\n",
       "   106                                                   elif window_idx == \"middle\":\n",
       "   107                                                       window_idx_offset = window / 2\n",
       "   108                                                   elif window_idx == \"begin\":\n",
       "   109                                                       window_idx_offset = 0\n",
       "   110                                                   else:\n",
       "   111                                                       raise ValueError(\n",
       "   112                                                           f\"window index {window_idx} must be either of: \"\n",
       "   113                                                           \"['end', 'middle', 'begin']\"\n",
       "   114                                                       )\n",
       "   115                                           \n",
       "   116                                                   # use closed = left to exclude 'end' if it falls on the boundary\n",
       "   117                                                   # note: the index automatically takes the timezone of `t_start` & `t_end`\n",
       "   118                                                   # note: the index-name of the first passed series will be used\n",
       "   119         1          3.0      3.0      0.2          self.index = pd.date_range(\n",
       "   120         1         18.0     18.0      1.0              start=t_start + window_idx_offset,\n",
       "   121         1         50.0     50.0      2.8              end=t_end - window + window_idx_offset,\n",
       "   122         1          3.0      3.0      0.2              freq=stride,\n",
       "   123         1        481.0    481.0     27.1              name=series_list[0].index.name\n",
       "   124                                                   )\n",
       "   125                                           \n",
       "   126                                                   # ---------- Efficient numpy code -------\n",
       "   127                                                   # 1. Convert everything to int64\n",
       "   128         1          8.0      8.0      0.5          np_start = t_start.to_datetime64()\n",
       "   129         1          6.0      6.0      0.3          np_window = self.window.to_timedelta64()\n",
       "   130         1          3.0      3.0      0.2          np_stride = self.stride.to_timedelta64()\n",
       "   131                                           \n",
       "   132                                                   # 2. Precompute the start & end times (these remain the same for each series)\n",
       "   133                                                   # note: this if equivalent to:\n",
       "   134                                                   #   if `window` == 'begin\":\n",
       "   135                                                   #       start_times = self.index.values\n",
       "   136         1          2.0      2.0      0.1          np_start_times = np.arange(\n",
       "   137         1         46.0     46.0      2.6              start=np_start, stop=np_start + (len(self.index) * np_stride),\n",
       "   138         1          3.0      3.0      0.2              step=np_stride,\n",
       "   139         1         11.0     11.0      0.6              dtype=np.datetime64,\n",
       "   140                                                   )\n",
       "   141         1         11.0     11.0      0.6          np_end_times = np_start_times + np_window\n",
       "   142                                           \n",
       "   143         1          3.0      3.0      0.2          self.series_containers: List[StridedRolling._NumpySeriesContainer] = []\n",
       "   144         2          5.0      2.5      0.3          for series in series_list:\n",
       "   145                                                       # create a non-writeable view of the series\n",
       "   146         1         13.0     13.0      0.7              np_series = series.values\n",
       "   147         1          8.0      8.0      0.5              np_series.flags.writeable = False\n",
       "   148                                           \n",
       "   149         1          5.0      5.0      0.3              np_idx_times = series.index.values\n",
       "   150         1          3.0      3.0      0.2              self.series_containers.append(\n",
       "   151         1          3.0      3.0      0.2                  StridedRolling._NumpySeriesContainer(\n",
       "   152                                                               # TODO: maybe save the pd.Series instead of the np.series\n",
       "   153         1          3.0      3.0      0.2                      values=np_series,\n",
       "   154                                                               # the slicing will be performed on [ t_start, t_end [\n",
       "   155                                                               # TODO: this can maybe be optimized -> further look into this\n",
       "   156                                                               # np_idx_times, np_start_times, & np_end_times are all sorted!\n",
       "   157                                                               # as we assume & check that the time index is monotonically\n",
       "   158                                                               # increasing & the latter 2 are created using `np.arange()`\n",
       "   159         1        184.0    184.0     10.4                      start_indexes=np.searchsorted(np_idx_times, np_start_times, \"left\"),\n",
       "   160         1         93.0     93.0      5.2                      end_indexes=np.searchsorted(np_idx_times, np_end_times, \"left\"),\n",
       "   161                                                           )\n",
       "   162                                                       )\n",
       "   163                                           \n",
       "   164         1          3.0      3.0      0.2              if not approve_sparsity:\n",
       "   165         1          3.0      3.0      0.2                  last_container = self.series_containers[-1]\n",
       "   166         1          4.0      4.0      0.2                  qs = [0, 0.1, 0.5, 0.9, 1]\n",
       "   167         1          2.0      2.0      0.1                  series_idx_stats = np.quantile(\n",
       "   168         1        476.0    476.0     26.8                      last_container.end_indexes - last_container.start_indexes, q=qs\n",
       "   169                                                           )\n",
       "   170         1         30.0     30.0      1.7                  q_str = \", \".join([f\"q={q}: {v}\" for q, v in zip(qs, series_idx_stats)])\n",
       "   171         1          4.0      4.0      0.2                  if series_idx_stats[0] != series_idx_stats[1]:  # min != max\n",
       "   172                                                               warnings.warn(\n",
       "   173                                                                   f\"There are gaps in the time-series {series.name}; \"\n",
       "   174                                                                   + f\"\\n \\t Quantiles of nb values in window: {q_str}\",\n",
       "   175                                                                   RuntimeWarning,\n",
       "   176                                                               )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f StridedRolling.__init__ StridedRolling(data=df_emg['emg'], window=window, stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9316409-fb6d-41ff-ba99-5952a2cb8a23",
   "metadata": {},
   "source": [
    "#### apply_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d1ca9d8-2333-41ca-aca5-e1c6fa0fc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroll = StridedRolling(df_emg['emg'], window, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b87fe0-997e-47a7-b385-2e56edf719f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: ../../tsflex/features/strided_rolling.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurences   Line Contents\n",
       "============================================================\n",
       "   220    317.1 MiB    317.1 MiB           1       def apply_func(self, np_func: Union[Callable, NumpyFuncWrapper]) -> pd.DataFrame:\n",
       "   221                                                 \"\"\"Apply a function to the expanded time-series.\n",
       "   222                                         \n",
       "   223                                                 Parameters\n",
       "   224                                                 ----------\n",
       "   225                                                 np_func : Union[Callable, NumpyFuncWrapper]\n",
       "   226                                                     The Callable (wrapped) function which will be applied.\n",
       "   227                                         \n",
       "   228                                                 Returns\n",
       "   229                                                 -------\n",
       "   230                                                 pd.DataFrame\n",
       "   231                                                     The merged output of the function applied to every column in a\n",
       "   232                                                     new DataFrame. The DataFrame's column-names have the format:\n",
       "   233                                                         `<series_col_name(s)>_<feature_name>__w=<window>_s=<stride>`.\n",
       "   234                                         \n",
       "   235                                                 Raises\n",
       "   236                                                 ------\n",
       "   237                                                 ValueError\n",
       "   238                                                     If the passed ``np_func`` tries to adjust the data its read-only view.\n",
       "   239                                         \n",
       "   240                                                 Notes\n",
       "   241                                                 -----\n",
       "   242                                                 * If ``np_func`` is only a callable argument, with no additional logic, this\n",
       "   243                                                   will only work for a one-to-one mapping, i.e., no multiple feature-output\n",
       "   244                                                   columns are supported for this case!<br>\n",
       "   245                                                   If you want to calculate one-to-many, ``np_func`` should be\n",
       "   246                                                   a ``NumpyFuncWrapper`` instance and explicitly use\n",
       "   247                                                   the ``output_names`` attributes of its constructor.\n",
       "   248                                         \n",
       "   249                                                 \"\"\"\n",
       "   250                                         \n",
       "   251                                                 # Convert win & stride to time-string if available :)\n",
       "   252    317.1 MiB      0.0 MiB           2           def create_feat_col_name(feat_name) -> str:\n",
       "   253    317.1 MiB      0.0 MiB           1               win_str = timedelta_to_str(self.window)\n",
       "   254    317.1 MiB      0.0 MiB           1               stride_str = timedelta_to_str(self.stride)\n",
       "   255    317.1 MiB      0.0 MiB           1               win_stride_str = f\"w={win_str}_s={stride_str}\"\n",
       "   256    317.1 MiB      0.0 MiB           1               return f\"{'|'.join(self.series_key)}__{feat_name}__{win_stride_str}\"\n",
       "   257                                         \n",
       "   258    317.1 MiB      0.0 MiB           1           if not isinstance(np_func, NumpyFuncWrapper):\n",
       "   259    317.1 MiB      0.0 MiB           1               np_func = NumpyFuncWrapper(np_func)\n",
       "   260    317.1 MiB      0.0 MiB           1           feat_names = np_func.output_names\n",
       "   261                                         \n",
       "   262    317.1 MiB      0.0 MiB           1           t_start = time.time()\n",
       "   263                                         \n",
       "   264    317.1 MiB      0.0 MiB         358           def get_slices(idx) -> List[np.array]:\n",
       "   265                                                     # get the slice of each series for the given index\n",
       "   266                                                     return [\n",
       "   267    317.1 MiB      0.0 MiB        1428                   sc.values[sc.start_indexes[idx]: sc.end_indexes[idx]]\n",
       "   268    317.1 MiB      0.0 MiB         714                   for sc in self.series_containers\n",
       "   269                                                     ]\n",
       "   270                                         \n",
       "   271                                                 # --- Future work ---\n",
       "   272                                                 # would be nice if we could optimize this double for loop with something\n",
       "   273                                                 # more vectorized\n",
       "   274                                                 # out = np.array(\n",
       "   275                                                 #     [np_func(\n",
       "   276                                                 #         *[sc.values[sc.start_indexes[idx]: sc.end_indexes[idx]]\n",
       "   277                                                 #           for sc in self.series_containers]\n",
       "   278                                                 #     ) for idx in range(len(self.index))]\n",
       "   279                                                 # )\n",
       "   280    317.1 MiB      0.0 MiB         360           out = np.array([np_func(*get_slices(idx)) for idx in range(len(self.index))])\n",
       "   281                                         \n",
       "   282                                                 # Aggregate function output in a dictionary\n",
       "   283    317.1 MiB      0.0 MiB           1           feat_out = {}\n",
       "   284    317.1 MiB      0.0 MiB           1           if out.ndim == 1 or (out.ndim == 2 and out.shape[1] == 1):\n",
       "   285    317.1 MiB      0.0 MiB           1               assert len(feat_names) == 1\n",
       "   286    317.1 MiB      0.0 MiB           1               feat_out[create_feat_col_name(feat_names[0])] = out.flatten()\n",
       "   287    317.1 MiB      0.0 MiB           1           if out.ndim == 2 and out.shape[1] > 1:\n",
       "   288                                                     assert len(feat_names) == out.shape[1]\n",
       "   289                                                     for col_idx in range(out.shape[1]):\n",
       "   290                                                         feat_out[create_feat_col_name(feat_names[col_idx])] = out[:, col_idx]\n",
       "   291                                         \n",
       "   292    317.1 MiB      0.0 MiB           1           elapsed = time.time() - t_start\n",
       "   293    317.1 MiB      0.0 MiB           1           logger.info(\n",
       "   294    317.1 MiB      0.0 MiB           1               f\"Finished function [{np_func.func.__name__}] on \"\n",
       "   295                                                     f\"{[self.series_key]} with window-stride \"\n",
       "   296                                                     f\"[{self.window}, {self.stride}] in [{elapsed} seconds]!\"\n",
       "   297                                                 )\n",
       "   298                                         \n",
       "   299    317.1 MiB      0.0 MiB           1           return pd.DataFrame(index=self.index, data=feat_out)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f stroll.apply_func stroll.apply_func(np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e8b9b7d-8cdd-4cec-a79d-1d16bf83b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 ms ± 17.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stroll.apply_func(np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15cb9343-7b38-4517-ac27-5fc7aca5877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.86 ms ± 19.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stroll.apply_func(np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56139546-d6d3-4261-aa90-2c50af85f7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.015861 s\n",
       "File: ../../tsflex/features/strided_rolling.py\n",
       "Function: apply_func at line 220\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   220                                               def apply_func(self, np_func: Union[Callable, NumpyFuncWrapper]) -> pd.DataFrame:\n",
       "   221                                                   \"\"\"Apply a function to the expanded time-series.\n",
       "   222                                           \n",
       "   223                                                   Parameters\n",
       "   224                                                   ----------\n",
       "   225                                                   np_func : Union[Callable, NumpyFuncWrapper]\n",
       "   226                                                       The Callable (wrapped) function which will be applied.\n",
       "   227                                           \n",
       "   228                                                   Returns\n",
       "   229                                                   -------\n",
       "   230                                                   pd.DataFrame\n",
       "   231                                                       The merged output of the function applied to every column in a\n",
       "   232                                                       new DataFrame. The DataFrame's column-names have the format:\n",
       "   233                                                           `<series_col_name(s)>_<feature_name>__w=<window>_s=<stride>`.\n",
       "   234                                           \n",
       "   235                                                   Raises\n",
       "   236                                                   ------\n",
       "   237                                                   ValueError\n",
       "   238                                                       If the passed ``np_func`` tries to adjust the data its read-only view.\n",
       "   239                                           \n",
       "   240                                                   Notes\n",
       "   241                                                   -----\n",
       "   242                                                   * If ``np_func`` is only a callable argument, with no additional logic, this\n",
       "   243                                                     will only work for a one-to-one mapping, i.e., no multiple feature-output\n",
       "   244                                                     columns are supported for this case!<br>\n",
       "   245                                                     If you want to calculate one-to-many, ``np_func`` should be\n",
       "   246                                                     a ``NumpyFuncWrapper`` instance and explicitly use\n",
       "   247                                                     the ``output_names`` attributes of its constructor.\n",
       "   248                                           \n",
       "   249                                                   \"\"\"\n",
       "   250                                           \n",
       "   251                                                   # Convert win & stride to time-string if available :)\n",
       "   252         1          4.0      4.0      0.0          def create_feat_col_name(feat_name) -> str:\n",
       "   253                                                       win_str = timedelta_to_str(self.window)\n",
       "   254                                                       stride_str = timedelta_to_str(self.stride)\n",
       "   255                                                       win_stride_str = f\"w={win_str}_s={stride_str}\"\n",
       "   256                                                       return f\"{'|'.join(self.series_key)}__{feat_name}__{win_stride_str}\"\n",
       "   257                                           \n",
       "   258         1          4.0      4.0      0.0          if not isinstance(np_func, NumpyFuncWrapper):\n",
       "   259         1         23.0     23.0      0.1              np_func = NumpyFuncWrapper(np_func)\n",
       "   260         1          2.0      2.0      0.0          feat_names = np_func.output_names\n",
       "   261                                           \n",
       "   262         1          3.0      3.0      0.0          t_start = time.time()\n",
       "   263                                           \n",
       "   264         1         19.0     19.0      0.1          def get_slices(idx) -> List[np.array]:\n",
       "   265                                                       # get the slice of each series for the given index\n",
       "   266                                                       return [\n",
       "   267                                                           sc.values[sc.start_indexes[idx]: sc.end_indexes[idx]]\n",
       "   268                                                           for sc in self.series_containers\n",
       "   269                                                       ]\n",
       "   270                                           \n",
       "   271                                                   # --- Future work ---\n",
       "   272                                                   # would be nice if we could optimize this double for loop with something\n",
       "   273                                                   # more vectorized\n",
       "   274                                                   # out = np.array(\n",
       "   275                                                   #     [np_func(\n",
       "   276                                                   #         *[sc.values[sc.start_indexes[idx]: sc.end_indexes[idx]]\n",
       "   277                                                   #           for sc in self.series_containers]\n",
       "   278                                                   #     ) for idx in range(len(self.index))]\n",
       "   279                                                   # )\n",
       "   280         1      13921.0  13921.0     87.8          out = np.array([np_func(*get_slices(idx)) for idx in range(len(self.index))])\n",
       "   281                                           \n",
       "   282                                                   # Aggregate function output in a dictionary\n",
       "   283         1          3.0      3.0      0.0          feat_out = {}\n",
       "   284         1          3.0      3.0      0.0          if out.ndim == 1 or (out.ndim == 2 and out.shape[1] == 1):\n",
       "   285         1          3.0      3.0      0.0              assert len(feat_names) == 1\n",
       "   286         1        205.0    205.0      1.3              feat_out[create_feat_col_name(feat_names[0])] = out.flatten()\n",
       "   287         1          2.0      2.0      0.0          if out.ndim == 2 and out.shape[1] > 1:\n",
       "   288                                                       assert len(feat_names) == out.shape[1]\n",
       "   289                                                       for col_idx in range(out.shape[1]):\n",
       "   290                                                           feat_out[create_feat_col_name(feat_names[col_idx])] = out[:, col_idx]\n",
       "   291                                           \n",
       "   292         1          3.0      3.0      0.0          elapsed = time.time() - t_start\n",
       "   293         1          4.0      4.0      0.0          logger.info(\n",
       "   294         1        213.0    213.0      1.3              f\"Finished function [{np_func.func.__name__}] on \"\n",
       "   295                                                       f\"{[self.series_key]} with window-stride \"\n",
       "   296                                                       f\"[{self.window}, {self.stride}] in [{elapsed} seconds]!\"\n",
       "   297                                                   )\n",
       "   298                                           \n",
       "   299         1       1449.0   1449.0      9.1          return pd.DataFrame(index=self.index, data=feat_out)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f stroll.apply_func stroll.apply_func(np.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae8780-dde3-40db-a5a2-c6b39fef9950",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### featurecollection testings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ecef2a-7291-41e1-8d8b-552689b6e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc = FeatureCollection(\n",
    "#     feature_descriptors=[\n",
    "#         MultipleFeatureDescriptors(\n",
    "#             functions=segment_funcs,\n",
    "#             series_names=[\"emg\", \"eog\", \"lso\", \"rio\", \"m1-a1\"],\n",
    "#             windows=[\"30s\"],\n",
    "#             strides=[\"10s\"],\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "# out = fc.calculate(data=df_emg, n_jobs=None, return_df=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38401c7b-04a5-4e84-ac74-88ec501d07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FeatureCollection(\n",
    "    feature_descriptors=[\n",
    "        MultipleFeatureDescriptors(\n",
    "            functions=segment_funcs,\n",
    "            series_names=[\"emg\", \"eog\", \"lso\", \"rio\", \"m1-a1\"],\n",
    "            windows=[\"30s\"],\n",
    "            strides=[\"10s\"],\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "627e7325-def4-45eb-8b3b-aa247da4bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c1f5bb-48bd-45f9-802f-7515dd8f17ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: ../../tsflex/features/feature_collection.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurences   Line Contents\n",
       "============================================================\n",
       "   187    302.1 MiB    302.1 MiB           1       def calculate(\n",
       "   188                                                 self,\n",
       "   189                                                 data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],\n",
       "   190                                                 return_df: Optional[bool] = False,\n",
       "   191                                                 window_idx: Optional[str] = 'end',\n",
       "   192                                                 approve_sparsity: Optional[bool] = False,\n",
       "   193                                                 show_progress: Optional[bool] = False,\n",
       "   194                                                 logging_file_path: Optional[Union[str, Path]] = None,\n",
       "   195                                                 n_jobs: Optional[int] = None,\n",
       "   196                                             ) -> Union[List[pd.DataFrame], pd.DataFrame]:\n",
       "   197                                                 \"\"\"Calculate features on the passed data.\n",
       "   198                                         \n",
       "   199                                                 Notes\n",
       "   200                                                 ------\n",
       "   201                                                 * The (column-)names of the series in `data` represent the names in the keys.\n",
       "   202                                                 * If a `logging_file_path` is provided, the execution (time) info can be\n",
       "   203                                                   retrieved by calling `logger.get_feature_logs(logging_file_path)`.\n",
       "   204                                                   Be aware that the `logging_file_path` gets cleared before the logger pushes\n",
       "   205                                                   logged messages. Hence, one should use a separate logging file for each\n",
       "   206                                                   constructed processing and feature instance with this library.\n",
       "   207                                         \n",
       "   208                                                 Parameters\n",
       "   209                                                 ----------\n",
       "   210                                                 data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]\n",
       "   211                                                     Dataframe or Series or list thereof, with all the required data for the\n",
       "   212                                                     feature calculation. \\n\n",
       "   213                                                     **Remark**: each Series / DataFrame must have a `pd.DatetimeIndex`.\n",
       "   214                                                     **Remark**: we assume that each name / column is unique.\n",
       "   215                                                 return_df : bool, optional\n",
       "   216                                                     Whether the output needs to be a dataframe list or a DataFrame, by default \n",
       "   217                                                     False.\n",
       "   218                                                     If `True` the output dataframes will be merged to a DataFrame with an outer\n",
       "   219                                                     merge.\n",
       "   220                                                 window_idx : str, optional\n",
       "   221                                                     The window's index position which will be used as index for the\n",
       "   222                                                     feature_window aggregation. Must be either of: ['begin', 'middle', 'end'],\n",
       "   223                                                     by default 'end'. All features in this collection will use the same\n",
       "   224                                                     window_idx.\n",
       "   225                                                 approve_sparsity: bool, optional\n",
       "   226                                                     Bool indicating whether the user acknowledges that there may be sparsity \n",
       "   227                                                     (i.e., irregularly sampled data), by default False.\n",
       "   228                                                     If False and sparsity is observed, a warning is raised.\n",
       "   229                                                 show_progress: bool, optional\n",
       "   230                                                     If True, the progress will be shown with a progressbar, by default False.\n",
       "   231                                                 logging_file_path : Union[str, Path], optional\n",
       "   232                                                     The file path where the logged messages are stored. If `None`, then no\n",
       "   233                                                     logging `FileHandler` will be used and the logging messages are only pushed\n",
       "   234                                                     to stdout. Otherwise, a logging `FileHandler` will write the logged messages\n",
       "   235                                                     to the given file path.\n",
       "   236                                                 n_jobs : int, optional\n",
       "   237                                                     The number of processes used for the feature calculation. If `None`, then\n",
       "   238                                                     the number returned by `os.cpu_count()` is used, by default None. \\n\n",
       "   239                                                     If n_jobs is either 0 or 1, the code will be executed sequentially without\n",
       "   240                                                     creating a process pool. This is very useful when debugging, as the stack\n",
       "   241                                                     trace will be more comprehensible.\n",
       "   242                                         \n",
       "   243                                                     .. tip::\n",
       "   244                                                         * It takes on avg. _300ms_ to schedule everything with\n",
       "   245                                                           multiprocessing. So if your feature extraction code runs faster than\n",
       "   246                                                           ~1.5s, it might not be worth it to parallelize the process\n",
       "   247                                                           (and thus better set the `n_jobs` to 0-1).\n",
       "   248                                                         * This method its memory peaks are significantly lower when executed\n",
       "   249                                                           sequentially. Set the `n_jobs` to 0-1 if this matters.\n",
       "   250                                         \n",
       "   251                                                 Returns\n",
       "   252                                                 -------\n",
       "   253                                                 Union[List[pd.DataFrame], pd.DataFrame]\n",
       "   254                                                     The calculated features.\n",
       "   255                                         \n",
       "   256                                                 Raises\n",
       "   257                                                 ------\n",
       "   258                                                 KeyError\n",
       "   259                                                     Raised when a required key is not found in `data`.\n",
       "   260                                         \n",
       "   261                                                 \"\"\"\n",
       "   262                                                 # Delete other logging handlers\n",
       "   263    302.1 MiB      0.0 MiB           1           delete_logging_handlers(logger)\n",
       "   264                                                 # Add logging handler (if path provided)\n",
       "   265    302.1 MiB      0.0 MiB           1           if logging_file_path:\n",
       "   266                                                     add_logging_handler(logger, logging_file_path)\n",
       "   267                                         \n",
       "   268                                                 # Convert the data to a series_dict\n",
       "   269    302.1 MiB      0.0 MiB           1           series_dict: Dict[str, pd.Series] = {}\n",
       "   270    302.1 MiB      0.0 MiB           6           for s in to_series_list(data):\n",
       "   271                                                     # Assert the assumptions we make!\n",
       "   272    302.1 MiB      0.0 MiB           5               assert isinstance(s.index, pd.DatetimeIndex)\n",
       "   273    302.1 MiB      0.0 MiB           5               assert s.index.is_monotonic_increasing\n",
       "   274                                         \n",
       "   275    302.1 MiB      0.0 MiB           5               if s.name in self.get_required_series():\n",
       "   276    302.1 MiB      0.0 MiB           5                   series_dict[str(s.name)] = s\n",
       "   277                                         \n",
       "   278    302.1 MiB      0.0 MiB           1           calculated_feature_list: List[pd.DataFrame] = []\n",
       "   279                                                 # Note: this variable has a global scope so this is shared in multiprocessing\n",
       "   280    302.1 MiB      0.0 MiB           1           if n_jobs in [0, 1]:\n",
       "   281                                                     # print('Executing feature extraction sequentially')\n",
       "   282    302.1 MiB      0.0 MiB           1               stroll_feat_generator = self._construct_stroll_feat_generator(\n",
       "   283    302.1 MiB      0.0 MiB           1                   series_dict, window_idx, approve_sparsity\n",
       "   284                                                     )\n",
       "   285    302.1 MiB      0.0 MiB           1               if show_progress:\n",
       "   286                                                         stroll_feat_generator = tqdm(stroll_feat_generator)\n",
       "   287    303.9 MiB      0.6 MiB          51               for stroll, func in stroll_feat_generator:\n",
       "   288    303.9 MiB      1.2 MiB          50                   calculated_feature_list.append(stroll.apply_func(func))\n",
       "   289                                                 else:\n",
       "   290                                                     global stroll_feat_list\n",
       "   291                                                     stroll_feat_list = self._construct_stroll_feat_list(\n",
       "   292                                                         series_dict, window_idx, approve_sparsity\n",
       "   293                                                     )\n",
       "   294                                                     # ---- Future work -----\n",
       "   295                                                     # Try locking inside the executer when calling next() on a global generator\n",
       "   296                                                     # Create global (precomputed) stroll-feature list \n",
       "   297                                                     # global stroll_feat_list\n",
       "   298                                                     # stroll_feat_list = [stroll_feat for stroll_feat in stroll_feat_generator]\n",
       "   299                                                     # https://pathos.readthedocs.io/en/latest/pathos.html#usage\n",
       "   300                                                     with ProcessPool(nodes=n_jobs, source=True) as pool:\n",
       "   301                                                         results = pool.uimap(\n",
       "   302                                                             self._executor,\n",
       "   303                                                             range(len(stroll_feat_list)),\n",
       "   304                                                         )\n",
       "   305                                                         if show_progress:\n",
       "   306                                                             results = tqdm(results, total=len(stroll_feat_list))\n",
       "   307                                                         calculated_feature_list = [f for f in results]\n",
       "   308                                                         # Close & join - see: https://github.com/uqfoundation/pathos/issues/131\n",
       "   309                                                         pool.close()\n",
       "   310                                                         pool.join()\n",
       "   311                                                         # Clear because: https://github.com/uqfoundation/pathos/issues/111\n",
       "   312                                                         pool.clear()\n",
       "   313                                         \n",
       "   314    303.9 MiB      0.0 MiB           1           if return_df:\n",
       "   315    304.1 MiB      0.2 MiB           1               return pd.concat(calculated_feature_list, axis=1, join='outer', copy=False)\n",
       "   316                                                 else:\n",
       "   317                                                     return calculated_feature_list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f fc.calculate fc.calculate(data=df_emg, n_jobs=1, return_df=True, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584ef20c-00f7-4cfa-bb02-ac7c23adf4a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: ../../tsflex/features/feature_collection.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurences   Line Contents\n",
       "============================================================\n",
       "   165    389.2 MiB    389.2 MiB           1       def calculate(\n",
       "   166                                                 self,\n",
       "   167                                                 data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],\n",
       "   168                                                 return_df: Optional[bool] = False,\n",
       "   169                                                 window_idx: Optional[str] = 'end',\n",
       "   170                                                 approve_sparsity: Optional[bool] = False,\n",
       "   171                                                 show_progress: Optional[bool] = False,\n",
       "   172                                                 logging_file_path: Optional[Union[str, Path]] = None,\n",
       "   173                                                 n_jobs: Optional[int] = None,\n",
       "   174                                             ) -> Union[List[pd.DataFrame], pd.DataFrame]:\n",
       "   175                                                 \"\"\"Calculate features on the passed data.\n",
       "   176                                         \n",
       "   177                                                 Notes\n",
       "   178                                                 ------\n",
       "   179                                                 * The (column-)names of the series in `data` represent the names in the keys.\n",
       "   180                                                 * If a `logging_file_path` is provided, the execution (time) info can be\n",
       "   181                                                   retrieved by calling `logger.get_feature_logs(logging_file_path)`.\n",
       "   182                                                   Be aware that the `logging_file_path` gets cleared before the logger pushes\n",
       "   183                                                   logged messages. Hence, one should use a separate logging file for each\n",
       "   184                                                   constructed processing and feature instance with this library.\n",
       "   185                                         \n",
       "   186                                                 Parameters\n",
       "   187                                                 ----------\n",
       "   188                                                 data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]\n",
       "   189                                                     Dataframe or Series or list thereof, with all the required data for the\n",
       "   190                                                     feature calculation. \\n\n",
       "   191                                                     **Remark**: each Series / DataFrame must have a `pd.DatetimeIndex`.\n",
       "   192                                                     **Remark**: we assume that each name / column is unique.\n",
       "   193                                                 return_df : bool, optional\n",
       "   194                                                     Whether the output needs to be a dataframe list or a DataFrame, by default \n",
       "   195                                                     False.\n",
       "   196                                                     If `True` the output dataframes will be merged to a DataFrame with an outer\n",
       "   197                                                     merge.\n",
       "   198                                                 window_idx : str, optional\n",
       "   199                                                     The window's index position which will be used as index for the\n",
       "   200                                                     feature_window aggregation. Must be either of: ['begin', 'middle', 'end'],\n",
       "   201                                                     by default 'end'. All features in this collection will use the same\n",
       "   202                                                     window_idx.\n",
       "   203                                                 approve_sparsity: bool, optional\n",
       "   204                                                     Bool indicating whether the user acknowledges that there may be sparsity \n",
       "   205                                                     (i.e., irregularly sampled data), by default False.\n",
       "   206                                                     If False and sparsity is observed, a warning is raised.\n",
       "   207                                                 show_progress: bool, optional\n",
       "   208                                                     If True, the progress will be shown with a progressbar, by default False.\n",
       "   209                                                 logging_file_path : Union[str, Path], optional\n",
       "   210                                                     The file path where the logged messages are stored. If `None`, then no\n",
       "   211                                                     logging `FileHandler` will be used and the logging messages are only pushed\n",
       "   212                                                     to stdout. Otherwise, a logging `FileHandler` will write the logged messages\n",
       "   213                                                     to the given file path.\n",
       "   214                                                 n_jobs : int, optional\n",
       "   215                                                     The number of processes used for the feature calculation. If `None`, then\n",
       "   216                                                     the number returned by `os.cpu_count()` is used, by default None. \\n\n",
       "   217                                                     If n_jobs is either 0 or 1, the code will be executed sequentially without\n",
       "   218                                                     creating a process pool. This is very useful when debugging, as the stack\n",
       "   219                                                     trace will be more comprehensible.\n",
       "   220                                         \n",
       "   221                                                     .. tip::\n",
       "   222                                                         * It takes on avg. _300ms_ to schedule everything with\n",
       "   223                                                           multiprocessing. So if your feature extraction code runs faster than\n",
       "   224                                                           ~1.5s, it might not be worth it to parallelize the process\n",
       "   225                                                           (and thus better set the `n_jobs` to 0-1).\n",
       "   226                                                         * This method its memory peaks are significantly lower when executed\n",
       "   227                                                           sequentially. Set the `n_jobs` to 0-1 if this matters.\n",
       "   228                                         \n",
       "   229                                                 Returns\n",
       "   230                                                 -------\n",
       "   231                                                 Union[List[pd.DataFrame], pd.DataFrame]\n",
       "   232                                                     The calculated features.\n",
       "   233                                         \n",
       "   234                                                 Raises\n",
       "   235                                                 ------\n",
       "   236                                                 KeyError\n",
       "   237                                                     Raised when a required key is not found in `data`.\n",
       "   238                                         \n",
       "   239                                                 \"\"\"\n",
       "   240                                                 # Delete other logging handlers\n",
       "   241    389.2 MiB      0.0 MiB           1           delete_logging_handlers(logger)\n",
       "   242                                                 # Add logging handler (if path provided)\n",
       "   243    389.2 MiB      0.0 MiB           1           if logging_file_path:\n",
       "   244                                                     add_logging_handler(logger, logging_file_path)\n",
       "   245                                         \n",
       "   246                                                 # Convert the data to a series_dict\n",
       "   247    389.2 MiB      0.0 MiB           1           series_dict: Dict[str, pd.Series] = {}\n",
       "   248    389.2 MiB      0.0 MiB           6           for s in to_series_list(data):\n",
       "   249                                                     # Assert the assumptions we make!\n",
       "   250    389.2 MiB      0.0 MiB           5               assert isinstance(s.index, pd.DatetimeIndex)\n",
       "   251    389.2 MiB      0.0 MiB           5               assert s.index.is_monotonic_increasing\n",
       "   252                                         \n",
       "   253    389.2 MiB      0.0 MiB           5               if s.name in self.get_required_series():\n",
       "   254    389.2 MiB      0.0 MiB           5                   series_dict[str(s.name)] = s\n",
       "   255                                         \n",
       "   256    389.2 MiB      0.0 MiB           1           calculated_feature_list: List[pd.DataFrame] = []\n",
       "   257                                                 # Note: this variable has a global scope so this is shared in multiprocessing\n",
       "   258                                                 global stroll_feat_list\n",
       "   259    389.2 MiB      0.0 MiB           1           stroll_feat_list = self._construct_stroll_feat_list(\n",
       "   260    389.2 MiB      0.0 MiB           1               series_dict, window_idx, approve_sparsity\n",
       "   261                                                 )\n",
       "   262                                         \n",
       "   263    389.2 MiB      0.0 MiB           1           if n_jobs in [0, 1]:\n",
       "   264                                                     # print('Executing feature extraction sequentially')\n",
       "   265                                                     if show_progress:\n",
       "   266                                                         stroll_feat_list = tqdm(stroll_feat_list)\n",
       "   267                                                     calculated_feature_list =[stroll.apply_func(func) for stroll, func in stroll_feat_list]\n",
       "   268                                                 else:\n",
       "   269                                                     # ---- Future work -----\n",
       "   270                                                     # Try locking inside the executer when calling next() on a global generator\n",
       "   271                                                     # Create global (precomputed) stroll-feature list \n",
       "   272                                                     # global stroll_feat_list\n",
       "   273                                                     # stroll_feat_list = [stroll_feat for stroll_feat in stroll_feat_generator]\n",
       "   274                                                     # https://pathos.readthedocs.io/en/latest/pathos.html#usage\n",
       "   275    389.2 MiB      0.0 MiB           1               with ProcessPool(nodes=n_jobs, source=True) as pool:\n",
       "   276    389.2 MiB      0.0 MiB           1                   results = pool.uimap(\n",
       "   277    389.2 MiB      0.0 MiB           1                       self._executor,\n",
       "   278    389.2 MiB      0.0 MiB           1                       range(len(stroll_feat_list)),\n",
       "   279                                                         )\n",
       "   280    389.2 MiB      0.0 MiB           1                   if show_progress:\n",
       "   281                                                             results = tqdm(results, total=len(stroll_feat_list))\n",
       "   282    389.2 MiB      0.0 MiB          53                   calculated_feature_list = [f for f in results]\n",
       "   283                                                         # Close & join - see: https://github.com/uqfoundation/pathos/issues/131\n",
       "   284    389.2 MiB      0.0 MiB           1                   pool.close()\n",
       "   285    389.2 MiB      0.0 MiB           1                   pool.join()\n",
       "   286                                                         # Clear because: https://github.com/uqfoundation/pathos/issues/111\n",
       "   287    389.2 MiB     -0.0 MiB           1                   pool.clear()\n",
       "   288                                         \n",
       "   289    389.2 MiB      0.0 MiB           1           if return_df:\n",
       "   290    389.2 MiB      0.0 MiB           1               return pd.concat(calculated_feature_list, axis=1, join='outer', copy=False)\n",
       "   291                                                 else:\n",
       "   292                                                     return calculated_feature_list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f fc.calculate fc.calculate(data=df_emg, n_jobs=10, return_df=True, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee22478-6e8b-40cc-bf8b-febb3a675f45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2071ee7f923d47dbb4a5eb6a925410c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: ../../tsflex/features/feature_collection.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurences   Line Contents\n",
       "============================================================\n",
       "   165    386.0 MiB    386.0 MiB           1       def calculate(\n",
       "   166                                                 self,\n",
       "   167                                                 data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],\n",
       "   168                                                 return_df: Optional[bool] = False,\n",
       "   169                                                 window_idx: Optional[str] = 'end',\n",
       "   170                                                 approve_sparsity: Optional[bool] = False,\n",
       "   171                                                 show_progress: Optional[bool] = False,\n",
       "   172                                                 logging_file_path: Optional[Union[str, Path]] = None,\n",
       "   173                                                 n_jobs: Optional[int] = None,\n",
       "   174                                             ) -> Union[List[pd.DataFrame], pd.DataFrame]:\n",
       "   175                                                 \"\"\"Calculate features on the passed data.\n",
       "   176                                         \n",
       "   177                                                 Notes\n",
       "   178                                                 ------\n",
       "   179                                                 * The (column-)names of the series in `data` represent the names in the keys.\n",
       "   180                                                 * If a `logging_file_path` is provided, the execution (time) info can be\n",
       "   181                                                   retrieved by calling `logger.get_feature_logs(logging_file_path)`.\n",
       "   182                                                   Be aware that the `logging_file_path` gets cleared before the logger pushes\n",
       "   183                                                   logged messages. Hence, one should use a separate logging file for each\n",
       "   184                                                   constructed processing and feature instance with this library.\n",
       "   185                                         \n",
       "   186                                                 Parameters\n",
       "   187                                                 ----------\n",
       "   188                                                 data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]\n",
       "   189                                                     Dataframe or Series or list thereof, with all the required data for the\n",
       "   190                                                     feature calculation. \\n\n",
       "   191                                                     **Remark**: each Series / DataFrame must have a `pd.DatetimeIndex`.\n",
       "   192                                                     **Remark**: we assume that each name / column is unique.\n",
       "   193                                                 return_df : bool, optional\n",
       "   194                                                     Whether the output needs to be a dataframe list or a DataFrame, by default \n",
       "   195                                                     False.\n",
       "   196                                                     If `True` the output dataframes will be merged to a DataFrame with an outer\n",
       "   197                                                     merge.\n",
       "   198                                                 window_idx : str, optional\n",
       "   199                                                     The window's index position which will be used as index for the\n",
       "   200                                                     feature_window aggregation. Must be either of: ['begin', 'middle', 'end'],\n",
       "   201                                                     by default 'end'. All features in this collection will use the same\n",
       "   202                                                     window_idx.\n",
       "   203                                                 approve_sparsity: bool, optional\n",
       "   204                                                     Bool indicating whether the user acknowledges that there may be sparsity \n",
       "   205                                                     (i.e., irregularly sampled data), by default False.\n",
       "   206                                                     If False and sparsity is observed, a warning is raised.\n",
       "   207                                                 show_progress: bool, optional\n",
       "   208                                                     If True, the progress will be shown with a progressbar, by default False.\n",
       "   209                                                 logging_file_path : Union[str, Path], optional\n",
       "   210                                                     The file path where the logged messages are stored. If `None`, then no\n",
       "   211                                                     logging `FileHandler` will be used and the logging messages are only pushed\n",
       "   212                                                     to stdout. Otherwise, a logging `FileHandler` will write the logged messages\n",
       "   213                                                     to the given file path.\n",
       "   214                                                 n_jobs : int, optional\n",
       "   215                                                     The number of processes used for the feature calculation. If `None`, then\n",
       "   216                                                     the number returned by `os.cpu_count()` is used, by default None. \\n\n",
       "   217                                                     If n_jobs is either 0 or 1, the code will be executed sequentially without\n",
       "   218                                                     creating a process pool. This is very useful when debugging, as the stack\n",
       "   219                                                     trace will be more comprehensible.\n",
       "   220                                         \n",
       "   221                                                     .. tip::\n",
       "   222                                                         * It takes on avg. _300ms_ to schedule everything with\n",
       "   223                                                           multiprocessing. So if your feature extraction code runs faster than\n",
       "   224                                                           ~1.5s, it might not be worth it to parallelize the process\n",
       "   225                                                           (and thus better set the `n_jobs` to 0-1).\n",
       "   226                                                         * This method its memory peaks are significantly lower when executed\n",
       "   227                                                           sequentially. Set the `n_jobs` to 0-1 if this matters.\n",
       "   228                                         \n",
       "   229                                                 Returns\n",
       "   230                                                 -------\n",
       "   231                                                 Union[List[pd.DataFrame], pd.DataFrame]\n",
       "   232                                                     The calculated features.\n",
       "   233                                         \n",
       "   234                                                 Raises\n",
       "   235                                                 ------\n",
       "   236                                                 KeyError\n",
       "   237                                                     Raised when a required key is not found in `data`.\n",
       "   238                                         \n",
       "   239                                                 \"\"\"\n",
       "   240                                                 # Delete other logging handlers\n",
       "   241    386.0 MiB      0.0 MiB           1           delete_logging_handlers(logger)\n",
       "   242                                                 # Add logging handler (if path provided)\n",
       "   243    386.0 MiB      0.0 MiB           1           if logging_file_path:\n",
       "   244                                                     add_logging_handler(logger, logging_file_path)\n",
       "   245                                         \n",
       "   246                                                 # Convert the data to a series_dict\n",
       "   247    386.0 MiB      0.0 MiB           1           series_dict: Dict[str, pd.Series] = {}\n",
       "   248    386.0 MiB      0.0 MiB           6           for s in to_series_list(data):\n",
       "   249                                                     # Assert the assumptions we make!\n",
       "   250    386.0 MiB      0.0 MiB           5               assert isinstance(s.index, pd.DatetimeIndex)\n",
       "   251    386.0 MiB      0.0 MiB           5               assert s.index.is_monotonic_increasing\n",
       "   252                                         \n",
       "   253    386.0 MiB      0.0 MiB           5               if s.name in self.get_required_series():\n",
       "   254    386.0 MiB      0.0 MiB           5                   series_dict[str(s.name)] = s\n",
       "   255                                         \n",
       "   256    386.0 MiB      0.0 MiB           1           calculated_feature_list: List[pd.DataFrame] = []\n",
       "   257                                                 # Note: this variable has a global scope so this is shared in multiprocessing\n",
       "   258                                                 global stroll_feat_list\n",
       "   259    386.0 MiB      0.0 MiB           1           stroll_feat_list = self._construct_stroll_feat_list(\n",
       "   260    386.0 MiB      0.0 MiB           1               series_dict, window_idx, approve_sparsity\n",
       "   261                                                 )\n",
       "   262                                         \n",
       "   263    386.0 MiB      0.0 MiB           1           if n_jobs in [0, 1]:\n",
       "   264                                                     # print('Executing feature extraction sequentially')\n",
       "   265                                                     if show_progress:\n",
       "   266                                                         stroll_feat_list = tqdm(stroll_feat_list)\n",
       "   267                                                     for stroll, func in stroll_feat_list:\n",
       "   268                                                         calculated_feature_list.append(stroll.apply_func(func))\n",
       "   269                                                 else:\n",
       "   270                                                     # ---- Future work -----\n",
       "   271                                                     # Try locking inside the executer when calling next() on a global generator\n",
       "   272                                                     # Create global (precomputed) stroll-feature list \n",
       "   273                                                     # global stroll_feat_list\n",
       "   274                                                     # stroll_feat_list = [stroll_feat for stroll_feat in stroll_feat_generator]\n",
       "   275                                                     # https://pathos.readthedocs.io/en/latest/pathos.html#usage\n",
       "   276    386.0 MiB      0.0 MiB           1               with ProcessPool(nodes=n_jobs, source=True) as pool:\n",
       "   277    386.0 MiB      0.0 MiB           1                   results = pool.uimap(\n",
       "   278    386.0 MiB      0.0 MiB           1                       self._executor,\n",
       "   279    386.0 MiB      0.0 MiB           1                       range(len(stroll_feat_list)),\n",
       "   280                                                         )\n",
       "   281    386.0 MiB      0.0 MiB           1                   if show_progress:\n",
       "   282    386.1 MiB      0.0 MiB           1                       results = tqdm(results, total=len(stroll_feat_list))\n",
       "   283    386.2 MiB      0.2 MiB          51                   for f in results:\n",
       "   284    386.2 MiB      0.0 MiB          50                       calculated_feature_list.append(f)\n",
       "   285                                                         # Close & join - see: https://github.com/uqfoundation/pathos/issues/131\n",
       "   286    386.2 MiB      0.0 MiB           1                   pool.close()\n",
       "   287    386.3 MiB      0.1 MiB           1                   pool.join()\n",
       "   288                                                         # Clear because: https://github.com/uqfoundation/pathos/issues/111\n",
       "   289    386.3 MiB     -0.0 MiB           1                   pool.clear()\n",
       "   290                                         \n",
       "   291    386.3 MiB      0.0 MiB           1           if return_df:\n",
       "   292    386.3 MiB      0.0 MiB           1               return pd.concat(calculated_feature_list, axis=1, join='outer', copy=False)\n",
       "   293                                                 else:\n",
       "   294                                                     return calculated_feature_list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f fc.calculate fc.calculate(data=df_emg, n_jobs=10, return_df=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a322f91c-40b7-49a6-8bda-cff6ecd60f8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3f614e7d62458a8a05725db3de766d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: ../../tsflex/features/feature_collection.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurences   Line Contents\n",
       "============================================================\n",
       "   165    383.1 MiB    383.1 MiB           1       def calculate(\n",
       "   166                                                 self,\n",
       "   167                                                 data: Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]],\n",
       "   168                                                 return_df: Optional[bool] = False,\n",
       "   169                                                 window_idx: Optional[str] = 'end',\n",
       "   170                                                 approve_sparsity: Optional[bool] = False,\n",
       "   171                                                 show_progress: Optional[bool] = False,\n",
       "   172                                                 logging_file_path: Optional[Union[str, Path]] = None,\n",
       "   173                                                 n_jobs: Optional[int] = None,\n",
       "   174                                             ) -> Union[List[pd.DataFrame], pd.DataFrame]:\n",
       "   175                                                 \"\"\"Calculate features on the passed data.\n",
       "   176                                         \n",
       "   177                                                 Notes\n",
       "   178                                                 ------\n",
       "   179                                                 * The (column-)names of the series in `data` represent the names in the keys.\n",
       "   180                                                 * If a `logging_file_path` is provided, the execution (time) info can be\n",
       "   181                                                   retrieved by calling `logger.get_feature_logs(logging_file_path)`.\n",
       "   182                                                   Be aware that the `logging_file_path` gets cleared before the logger pushes\n",
       "   183                                                   logged messages. Hence, one should use a separate logging file for each\n",
       "   184                                                   constructed processing and feature instance with this library.\n",
       "   185                                         \n",
       "   186                                                 Parameters\n",
       "   187                                                 ----------\n",
       "   188                                                 data : Union[pd.Series, pd.DataFrame, List[Union[pd.Series, pd.DataFrame]]]\n",
       "   189                                                     Dataframe or Series or list thereof, with all the required data for the\n",
       "   190                                                     feature calculation. \\n\n",
       "   191                                                     **Remark**: each Series / DataFrame must have a `pd.DatetimeIndex`.\n",
       "   192                                                     **Remark**: we assume that each name / column is unique.\n",
       "   193                                                 return_df : bool, optional\n",
       "   194                                                     Whether the output needs to be a dataframe list or a DataFrame, by default \n",
       "   195                                                     False.\n",
       "   196                                                     If `True` the output dataframes will be merged to a DataFrame with an outer\n",
       "   197                                                     merge.\n",
       "   198                                                 window_idx : str, optional\n",
       "   199                                                     The window's index position which will be used as index for the\n",
       "   200                                                     feature_window aggregation. Must be either of: ['begin', 'middle', 'end'],\n",
       "   201                                                     by default 'end'. All features in this collection will use the same\n",
       "   202                                                     window_idx.\n",
       "   203                                                 approve_sparsity: bool, optional\n",
       "   204                                                     Bool indicating whether the user acknowledges that there may be sparsity \n",
       "   205                                                     (i.e., irregularly sampled data), by default False.\n",
       "   206                                                     If False and sparsity is observed, a warning is raised.\n",
       "   207                                                 show_progress: bool, optional\n",
       "   208                                                     If True, the progress will be shown with a progressbar, by default False.\n",
       "   209                                                 logging_file_path : Union[str, Path], optional\n",
       "   210                                                     The file path where the logged messages are stored. If `None`, then no\n",
       "   211                                                     logging `FileHandler` will be used and the logging messages are only pushed\n",
       "   212                                                     to stdout. Otherwise, a logging `FileHandler` will write the logged messages\n",
       "   213                                                     to the given file path.\n",
       "   214                                                 n_jobs : int, optional\n",
       "   215                                                     The number of processes used for the feature calculation. If `None`, then\n",
       "   216                                                     the number returned by `os.cpu_count()` is used, by default None. \\n\n",
       "   217                                                     If n_jobs is either 0 or 1, the code will be executed sequentially without\n",
       "   218                                                     creating a process pool. This is very useful when debugging, as the stack\n",
       "   219                                                     trace will be more comprehensible.\n",
       "   220                                         \n",
       "   221                                                     .. tip::\n",
       "   222                                                         * It takes on avg. _300ms_ to schedule everything with\n",
       "   223                                                           multiprocessing. So if your feature extraction code runs faster than\n",
       "   224                                                           ~1.5s, it might not be worth it to parallelize the process\n",
       "   225                                                           (and thus better set the `n_jobs` to 0-1).\n",
       "   226                                                         * This method its memory peaks are significantly lower when executed\n",
       "   227                                                           sequentially. Set the `n_jobs` to 0-1 if this matters.\n",
       "   228                                         \n",
       "   229                                                 Returns\n",
       "   230                                                 -------\n",
       "   231                                                 Union[List[pd.DataFrame], pd.DataFrame]\n",
       "   232                                                     The calculated features.\n",
       "   233                                         \n",
       "   234                                                 Raises\n",
       "   235                                                 ------\n",
       "   236                                                 KeyError\n",
       "   237                                                     Raised when a required key is not found in `data`.\n",
       "   238                                         \n",
       "   239                                                 \"\"\"\n",
       "   240                                                 # Delete other logging handlers\n",
       "   241    383.1 MiB      0.0 MiB           1           delete_logging_handlers(logger)\n",
       "   242                                                 # Add logging handler (if path provided)\n",
       "   243    383.1 MiB      0.0 MiB           1           if logging_file_path:\n",
       "   244                                                     add_logging_handler(logger, logging_file_path)\n",
       "   245                                         \n",
       "   246                                                 # Convert the data to a series_dict\n",
       "   247    383.1 MiB      0.0 MiB           1           series_dict: Dict[str, pd.Series] = {}\n",
       "   248    383.1 MiB      0.0 MiB           6           for s in to_series_list(data):\n",
       "   249                                                     # Assert the assumptions we make!\n",
       "   250    383.1 MiB      0.0 MiB           5               assert isinstance(s.index, pd.DatetimeIndex)\n",
       "   251    383.1 MiB      0.1 MiB           5               assert s.index.is_monotonic_increasing\n",
       "   252                                         \n",
       "   253    383.1 MiB      0.0 MiB           5               if s.name in self.get_required_series():\n",
       "   254    383.1 MiB      0.0 MiB           5                   series_dict[str(s.name)] = s\n",
       "   255                                         \n",
       "   256    383.1 MiB      0.0 MiB           1           calculated_feature_list: List[pd.DataFrame] = []\n",
       "   257                                                 # Note: this variable has a global scope so this is shared in multiprocessing\n",
       "   258                                                 global stroll_feat_list\n",
       "   259    383.1 MiB      0.0 MiB           1           stroll_feat_list = self._construct_stroll_feat_list(\n",
       "   260    383.5 MiB      0.3 MiB           1               series_dict, window_idx, approve_sparsity\n",
       "   261                                                 )\n",
       "   262                                         \n",
       "   263    383.5 MiB      0.0 MiB           1           if n_jobs in [0, 1]:\n",
       "   264                                                     # print('Executing feature extraction sequentially')\n",
       "   265                                                     if show_progress:\n",
       "   266                                                         stroll_feat_list = tqdm(stroll_feat_list)\n",
       "   267                                                     for stroll, func in stroll_feat_list:\n",
       "   268                                                         calculated_feature_list.append(stroll.apply_func(func))\n",
       "   269                                                 else:\n",
       "   270                                                     # ---- Future work -----\n",
       "   271                                                     # Try locking inside the executer when calling next() on a global generator\n",
       "   272                                                     # Create global (precomputed) stroll-feature list \n",
       "   273                                                     # global stroll_feat_list\n",
       "   274                                                     # stroll_feat_list = [stroll_feat for stroll_feat in stroll_feat_generator]\n",
       "   275                                                     # https://pathos.readthedocs.io/en/latest/pathos.html#usage\n",
       "   276    383.8 MiB      0.3 MiB           1               with ProcessPool(nodes=n_jobs, source=True) as pool:\n",
       "   277    383.8 MiB      0.0 MiB           1                   results = pool.uimap(\n",
       "   278    383.8 MiB      0.0 MiB           1                       self._executor,\n",
       "   279    383.8 MiB      0.0 MiB           1                       range(len(stroll_feat_list)),\n",
       "   280                                                         )\n",
       "   281    383.8 MiB      0.0 MiB           1                   if show_progress:\n",
       "   282    384.1 MiB      0.2 MiB           1                       results = tqdm(results, total=len(stroll_feat_list))\n",
       "   283    384.7 MiB      0.7 MiB          51                   for f in results:\n",
       "   284    384.4 MiB      0.0 MiB          50                       calculated_feature_list.append(f)\n",
       "   285                                                         # Close & join - see: https://github.com/uqfoundation/pathos/issues/131\n",
       "   286    384.7 MiB      0.0 MiB           1                   pool.close()\n",
       "   287    384.9 MiB      0.1 MiB           1                   pool.join()\n",
       "   288                                                         # Clear because: https://github.com/uqfoundation/pathos/issues/111\n",
       "   289    384.9 MiB      0.0 MiB           1                   pool.clear()\n",
       "   290                                         \n",
       "   291    384.9 MiB      0.0 MiB           1           if return_df:\n",
       "   292    384.9 MiB      0.0 MiB           1               df_merged = pd.DataFrame()\n",
       "   293    385.4 MiB      0.0 MiB          51               for calculated_feature in calculated_feature_list:\n",
       "   294    385.4 MiB      0.0 MiB          50                   df_merged = pd.merge(\n",
       "   295    385.4 MiB      0.0 MiB          50                       left=df_merged,\n",
       "   296    385.4 MiB      0.0 MiB          50                       right=calculated_feature,\n",
       "   297    385.4 MiB      0.0 MiB          50                       how=\"outer\",\n",
       "   298    385.4 MiB      0.0 MiB          50                       left_index=True,\n",
       "   299    385.4 MiB      0.5 MiB          50                       right_index=True,\n",
       "   300                                                         )\n",
       "   301    385.4 MiB      0.0 MiB           1               return df_merged\n",
       "   302                                                 else:\n",
       "   303                                                     return calculated_feature_list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f fc.calculate fc.calculate(data=df_emg, n_jobs=10, return_df=True, show_progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
